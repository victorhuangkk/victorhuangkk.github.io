---
layout: post
title: TD Proposal
tags: [other]
comments: true
---


## Summary

In this internship, I am trying to provide end-to-end machine learning solutions to help traders improve their performance over time. Technical trading exist for a long time, however, human beings may not be able to conceive all the information as fast as machine. Simultaneously, human might be biased towards what they believe. So, quantitative models have been adopted in the finance world, to help traders

## Time Series Forecasting
Time series is a long lasting field in statistical research. From auto regressive to moving average. Then, ARIMA and VAR came out. The later one is able to incorporate multiple features all together. That is also the main drawback of traditional time series models, which cannot utilize contextual features. To utilize all available features and detect profitable trading opportunities, we embrace deep neural network approach. After AlexNet being published, DNN received tons of research effort in the past eight years.

My proposal is to build a few models based on several literatures. The question we are answering here is how to accurately predict beta adjusted yield curve in the near future. Our belief is that beside information contains in yield curve itself, other macro factors, even price in other asset classes may hugely impact the yield curve. Compared with more traditional approaches like VAR(Vector Autoregression) or XGBoost(Extreme Gradient Boosting), deep learning wins because of supreme learning ability. And it doesn't rely on feature engineering, which determines performance of most statistical machine learning models.

The engineering details include but not limited to:
1. Use monthly data to train and evolve models. Inside each training dataset, conduct log transformation to normalize variables.
2. Borrow empirical expertise from team members to extract as many useful features as possible from Bloomberg.
3. Modeling. I would utilize RNN autoencoder + DNN regression. RNN autoencoder is used to deduce dimension, feature extraction. And learning part is done by DNN regression.

The modeling part can be tweaked by switching to LSTM model and Transformer Model. Also, features can be added by the following section's text information. 

## Natural Language Processing
Natural language processing (NLP) is two of the most widely used machine learning technics, the other one is computer vision. Considering all the data we have on hand now, they are all numerical. In some sense, numerical data is biased and very limited. Deep learning is powerful when you give it enough volume of data. Considering the well known AlexNet, they applied deep neural network on image data and dwarfed all the other approaches. Reasons are they have 1.2 million high resolution images and train a model with 60 million parameters. And supremacy of deep learning is set on top of big data. So, in this part, our purpose is to extract as much information as possible from text data.

My plan is to build multiple text related features based on text data, and add those extra information to our time series models. To make the example more concrete, let's focus on annually company report. Stakeholders would cover multiple topics, and their wording have sentiment, and numerical data mentioned in the report are all available information. Besides 10K, 10Q reports, we may explore social network as well, including Facebook, Twitter and Reddit. After collecting all useful text data, Bloomberg ID and time would be used as the unique identifier for each record. Then, I will process and organize the data, conduct LDA(Latent Dirichlet Allocation) to do topic modeling, and BERT (the Google SOTA NLP model) to conduct sentiment analysis and further feature engineering. Basically, the plan is to embed more predictors to the time series model to unleash deep learning model.  


## Reinforcement Learning Trading
The final goal of trading is to generate profit and alleviate risk. We are the trading team, such that passive investment (portfolio optimization) may not be the good direction. In the recent ten years, an increasingly amount of efforts have been devoted to apply Reinforcement Learning to quantitative trading. In general, there are a few key components in reinforcement learning which match their counterparts in finance greatly. For example, environment is term in reinforcement learning, corresponding to all the unknown information. And in finance, this term naturally matches with the generic financial market. And to make this design doc project specific, I would skip the theoretical explanations and jump directly to model selection and execution plan.

To complement this project in a reasonable time frame, I choose to use a relatively simple reinforcement technic, Thompson Sampling, which was developed to solve multi arms bandit (MAB) problem. MAB's scenario is that you have multiple slot machines in front of you, each of them have different odds to win. Your task is to maximize your profit in a limited time. And the key pain points in this problem is to balance exploration and exploitation. By having trading odds given by the time series model, traders know they have to long/short certain assets. However, how many shares should they trade; should they hold or wait? All of these questions could be answered by the reinforcement learning model. Or, at least, help traders to function more effectively.

The plan is to backtest (next section) multiple strategies generated by the time series model by allocating different weights to autoregressive factor, natural language factor and potentially other factors. Each strategy is a "slot machine" in our application and our goal is to modify each strategy and find out the strategy with highest return.

## Backtest

In order to optimize multiple strategies, backtest is one of the common technics. Basically, the model is trained based on historical data, and assume we back to the history. We use the near recent data to "trade" and test our strategies. To automate this process, I prefer a python framework provided by Quantopian. Based on my previous experience, we should be able to test our algorithms based on this framework. My plan is to prototype this part since it is not our priority in this project. But I would love it to be developed to prove to the stakeholders that algorithms really worked.


## Visualization and Dashboarding
For presentation purposes, a python dashboard (written in Dash) would be provided to provide data insights. Compared with static slides, end users would be able to pull data based on their interest. And for development purposes, this dashboard is useful for debugging purposes. Technical details would be skipped in this design doc, but I would love to list out some keys features of this dashboard.

1. It is multi-tab, enable user to easily search for information that they have interest to.
2. It is Dash native. As little dependencies are used as possible to avoid confusion.
3. Multi sessions are enabled. Based on the current design, it should support at most ten users to interact with the dashboard simultaneously. User's data are isolated and would not be shared cross session.

For further implementation details, please refer to this repo on my deepgreen folder.
